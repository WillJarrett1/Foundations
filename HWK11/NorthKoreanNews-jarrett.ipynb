{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# North Korean News\n",
    "\n",
    "Scrape the North Korean news agency http://kcna.kp\n",
    "\n",
    "Save a CSV called `nk-news.csv`. This file should include:\n",
    "\n",
    "* The **article headline**\n",
    "* The value of **`onclick`** (they don't have normal links)\n",
    "* The **article ID** (for example, the article ID for `fn_showArticle(\"AR0125885\", \"\", \"NT00\", \"L\")` is `AR0125885`\n",
    "\n",
    "The last part is easiest using pandas. Be sure you don't save the index!\n",
    "\n",
    "* _**Tip:** If you're using requests+BeautifulSoup, you can always look at response.text to see if the page looks like what you think it looks like_\n",
    "* _**Tip:** Check your URL to make sure it is what you think it should be!_\n",
    "* _**Tip:** Does it look different if you scrape with BeautifulSoup compared to if you scrape it with Selenium?_\n",
    "* _**Tip:** For the last part, how do you pull out part of a string from a longer string?_\n",
    "* _**Tip:** `expand=False` is helpful if you want to assign a single new column when extracting_\n",
    "* _**Tip:** `(` and `)` mean something special in regular expressions, so you have to say \"no really seriously I mean `(`\" by using `\\(` instead_\n",
    "* _**Tip:** if your `.*` is taking up too much stuff, you can try `.*?` instead, which instead of \"take as much as possible\" it means \"take only as much as needed\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('http://kcna.kp/kcna.user.home.retrieveHomeInfoList.kcmsf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Translate page to English\n",
    "translate = driver.find_element_by_xpath('/html/body/center/div[1]/div[1]/div/div[2]/p/a[2]')\n",
    "translate.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stick elements in list of dictionaries\n",
    "all_articles = []\n",
    "articles = driver.find_elements_by_class_name(\"titlebet\")\n",
    "\n",
    "for row in articles:\n",
    "    headline = row.text\n",
    "    onclick = row.get_attribute('onclick')\n",
    "    article_id = re.search('fn_showArticle\\(\"(.{9}).*', onclick).group(1)\n",
    "    article_dict = {\n",
    "        'headline': headline,\n",
    "        'onclick': onclick,\n",
    "        'article_id': article_id\n",
    "    }\n",
    "    all_articles.append(article_dict)\n",
    "        \n",
    "all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get rid of empty rows\n",
    "df = pd.DataFrame(all_articles)\n",
    "df = df[df.headline != \"\"].reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('nk-news.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
