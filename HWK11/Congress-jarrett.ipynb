{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping one page per row\n",
    "\n",
    "Let's say we're interested in our members of Congress, because who isn't? Read in `congress.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load from CSV\n",
    "df = pd.read_csv('congress.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)' \\\n",
    "    'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's scrape one\n",
    "\n",
    "The `slug` is the part of the URL that's particular to that member of Congress. So `/james-abdnor/A000009` really means `https://www.congress.gov/member/james-abdnor/A000009`.\n",
    "\n",
    "Scrape his name, birthdaye, party, whether he's currently in congress, and his bill count (don't worry if the bill count is dirty, you can clean it up later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.congress.gov/member/steven-palazzo/P000601\"\n",
    "html = requests.get(url, headers=headers).content\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Name\n",
    "name = re.search(r\"^\\w+\\s(.+)\", soup.find(class_='legDetail').next).group(1)\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Senate or rep?\n",
    "sen_rep = re.search(r\"^(\\w+)\\s.+\", soup.find(class_='legDetail').next).group(1)\n",
    "sen_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Birth year\n",
    "birth_year = re.search(r\"^\\((\\d\\d\\d\\d)\", soup.find(class_='birthdate').string.strip()).group(1)\n",
    "birth_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Party\n",
    "party = soup.find(class_='member_party').next_sibling.next_sibling.string\n",
    "party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Current member?\n",
    "try:\n",
    "    end_of_term = re.search(r\"(.{7}$)\", soup.find(class_='birthdate').next_sibling.string.strip()).group(1)\n",
    "    if end_of_term == \"Present\":\n",
    "        current_member = \"Yes\"\n",
    "    else:\n",
    "        current_member = \"No\"\n",
    "except:\n",
    "    current_member = \"ERROR\"\n",
    "current_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of bills\n",
    "bills = re.search(r\"^of (.+)$\", soup.find(class_=\"results-number\").find('strong').next.next.strip()).group(1).replace(\",\", \"\")\n",
    "bills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a function\n",
    "\n",
    "Write a function called `scrape_page` that makes a URL out of the the `slug`, like we're going to use `.apply`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_urls = []\n",
    "def scrape_page(slug):\n",
    "    whole_urls.append(\"https://www.congress.gov/member/\" + slug)\n",
    "    \n",
    "df['slug'].apply(scrape_page)\n",
    "whole_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the scraping\n",
    "\n",
    "Rewrite `scrape_page` to actually scrape the URL. You can use your scraping code from up above. Start by testing with just one row (I put a sample call below) and then expand to your whole dataframe.\n",
    "\n",
    "Save the results as `scraped_df`.\n",
    "\n",
    "* **Hint:** Be sure to use `return`!\n",
    "* **Hint:** Make sure you return a `pd.Series`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_rows = []\n",
    "\n",
    "def scrape_page(slug):\n",
    "    try:\n",
    "        #Get URLs\n",
    "        url = \"https://www.congress.gov/member/\" + slug\n",
    "        html = requests.get(url, headers=headers).content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        #Scrape\n",
    "        name = re.search(r\"^\\w+\\s(.+)\", soup.find(class_='legDetail').next).group(1)\n",
    "        sen_rep = re.search(r\"^(\\w+)\\s.+\", soup.find(class_='legDetail').next).group(1)\n",
    "        birth_year = re.search(r\"^\\((\\d\\d\\d\\d)\", soup.find(class_='birthdate').string.strip()).group(1)\n",
    "        if soup.find(class_='member_party') is None:\n",
    "            party = soup.find(class_='member_party_history').next_sibling.next_sibling.text.strip()\n",
    "        else:\n",
    "            party = soup.find(class_='member_party').next_sibling.next_sibling.string\n",
    "        end_of_term = soup.find(class_='birthdate').next_sibling.text.strip()\n",
    "        if \"Present\" in end_of_term:\n",
    "            current_member = \"Yes\"\n",
    "        else:\n",
    "            current_member = \"No\"\n",
    "        bills = re.search(r\"^of (.+)$\", soup.find(class_=\"results-number\").find('strong').next.next.strip()).group(1).replace(\",\", \"\")\n",
    "\n",
    "        #Put into dictionary\n",
    "        rows_dict = {\n",
    "        'name': name,\n",
    "        'sen_rep': sen_rep,\n",
    "        'birth_year': birth_year,\n",
    "        'party': party,\n",
    "        'current_member': current_member,\n",
    "        'bills': bills,\n",
    "        'url': url,\n",
    "        }\n",
    "\n",
    "        all_rows.append(rows_dict)\n",
    "        print(url)\n",
    "    except:\n",
    "        rows_dict = {\n",
    "        'name': 'ERROR',\n",
    "        'sen_rep': 'ERROR',\n",
    "        'birth_year': 'ERROR',\n",
    "        'party': 'ERROR',\n",
    "        'current_member': 'ERROR',\n",
    "        'bills': 'ERROR',\n",
    "        'url': 'ERROR',\n",
    "        }\n",
    "        all_rows.append(rows_dict)\n",
    "        print(url)\n",
    "    return\n",
    "\n",
    "#Set off function\n",
    "df['slug'].apply(scrape_page)\n",
    "all_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join with your original dataframe\n",
    "\n",
    "Join your new data with your original data, adding the `_scraped` suffix on the new columns. You can use either `.join` or `.merge`, but be sure to read the docs to know the difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#My code keeps getting stuck about halfway through. No error message, just no more progress.\n",
    "#I will come back to this and have a go at chopping the csv into smaller pieces to make it more manageable,\n",
    "#but for now I am going to move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save it\n",
    "\n",
    "Save your combined results to `congress-plus-scraped.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
